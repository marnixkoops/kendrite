# general params
seed: 0

# engineering params
target: "class"
exclude_cols: null
test_size: 0.2
val_size: 0.2

# model params
train_tabnet:
    estimator:
        task: "classification" # regression" or "classification"
        kwargs:
            n_d: 48
            n_a: 48
            n_steps: 6
            gamma: 1.5
            n_independent: 2
            n_shared: 2
            epsilon: 1.0e-15
            momentum: 0.3
            lambda_sparse: 0.01
            seed: 0
            clip_value: 1
            verbose: 1
            optimizer_fn: "torch.optim.Adam"
            optimizer_params:
                lr: 0.02
            scheduler_fn: "torch.optim.lr_scheduler.StepLR"
            scheduler_params:
                gamma: 0.95
                step_size: 20
            mask_type: "sparsemax"
            device_name: "auto"
    fit:
        eval_metric: ["accuracy", "balanced_accuracy", "logloss"]
        weights: 1
        max_epochs: &epoch 64
        patience: 64
        batch_size: 256
        callbacks:
            - class: kendrite.pipelines.tuning.TuneReportCallback
              kwargs:
                  metrics: &monitor "valid_logloss"
                  "on": "epoch_end"
    tune_params:
        metric: *monitor
        mode: min
        local_dir: data/07_model_output/ray_tune_results/ # store tune results
        fail_fast: True
        config:
            n_d:
                class: ray.tune.randint
                kwargs:
                    lower: 4
                    upper: 64
            n_a:
                class: ray.tune.randint
                kwargs:
                    lower: 4
                    upper: 64
            n_steps:
                class: ray.tune.randint
                kwargs:
                    lower: 3
                    upper: 10
            gamma:
                class: ray.tune.uniform
                kwargs:
                    lower: !!float 1.0
                    upper: !!float 2.0
        scheduler:
            class: ray.tune.schedulers.AsyncHyperBandScheduler
            kwargs:
                time_attr: training_iteration
                max_t: *epoch
                grace_period: 20
        search_alg:
            class: ray.tune.suggest.optuna.OptunaSearch
            kwargs: {}
        # callbacks: - List[Dict]. Each dict-> class and kwarg: Dict[str,str]
        num_samples: 20
        verbose: 1 # level of messages to print
        stop:
            training_iteration: *epoch
        resources_per_trial:
            cpu: 1
            gpu: 0
